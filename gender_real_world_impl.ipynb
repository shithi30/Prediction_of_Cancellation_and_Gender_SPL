{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector \n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_orig=genfromtxt('gender_14000_preprocessed.csv', delimiter=',') \n",
    "XY_orig=np.matrix(XY_orig)          # original .csv dataset\n",
    "X_orig=XY_orig[1:14001, 0:30].T     # eliminating feature-names, labels, unnecessary features and transpose to get X_orig\n",
    "Y_orig=XY_orig[1:14001, 30].T       # extracting labels and transpose to get Y\n",
    "X_orig_max = X_orig.max(1)          # normalizing features\n",
    "X_orig=X_orig/X_orig_max\n",
    "\n",
    "X_train=X_orig[:, 0:11200]          # splitting into X_train\n",
    "Y_train=Y_orig[:, 0:11200]          # splitting into Y_train\n",
    "\n",
    "X_test=X_orig[:, 11200:14000]       # splitting into X_test\n",
    "Y_test=Y_orig[:, 11200:14000]       # splitting into Y_test\n",
    "\n",
    "X_train=np.array(X_train)           # converting to array from matrices\n",
    "X_test=np.array(X_test)\n",
    "Y_train=np.array(Y_train)\n",
    "Y_test=np.array(Y_test)\n",
    "\n",
    "Y_train=Y_train.astype(int)         # converting to one-hot\n",
    "Y_test=Y_test.astype(int)\n",
    "Y_train=convert_to_one_hot(Y_train, 2)\n",
    "Y_test=convert_to_one_hot(Y_test, 2)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))      \n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to DB\n",
    "conn=mysql.connector.connect(\n",
    "    host='sheba-rr.cifg1gfqorjf.ap-south-1.rds.amazonaws.com',\n",
    "    user='shafiq',\n",
    "    passwd='Shafiq|Sheba#10213223',\n",
    "    database='sheba'\n",
    ")\n",
    "\n",
    "# create cursor\n",
    "try: cursor=conn.cursor()\n",
    "except: print(\"Error in creating cursor!\")\n",
    "    \n",
    "# prepare query for customers to be pitched\n",
    "sql_query=\"\"\"-- customers who took service yesterday and may be cross-sold into BTY\n",
    "SELECT *\n",
    "FROM\n",
    "    (-- customers who took service yesterday\n",
    "    SELECT DISTINCT\n",
    "        customer_id, UPPER(customer_name) customer_name, customer_mobile, services,\n",
    "        CASE\n",
    "            WHEN master_category_id IN(1, 2, 73, 84, 91, 225, 240,624) THEN 'HAB'\n",
    "            WHEN master_category_id IN(226, 416, 619) THEN 'REN'\n",
    "            WHEN master_category_id IN(3, 4, 5, 6, 221, 224, 235, 236, 365, 505, 596, 599, 621, 635) THEN 'TRV'\n",
    "            WHEN master_category_id IN(183, 334, 544) THEN 'BTY'\n",
    "            WHEN master_category_id IN(8, 186, 237) THEN 'OHR'\n",
    "            WHEN master_category_id IN(185, 266) THEN 'FOD'\n",
    "            WHEN master_category_id IN(184) THEN 'LDY'\n",
    "            WHEN master_category_id IN(7, 9, 94, 101, 104, 123, 133, 286, 287, 288, 289, 290, 291, 296, 317, 333, 335, 336, 337, 338, 344, 348, 353,387, 391, 411, 441, 433, 455, 537, 549, 581, 627, 636) THEN 'NEW'\n",
    "        END SBU\n",
    "    FROM partner_order_report\n",
    "    WHERE DATE(closed_date)=DATE_ADD(DATE(NOW()), INTERVAL -1 DAY)) A\n",
    "WHERE SBU IN('HAB', 'OHR', 'FOD', 'LDY')\"\"\"\n",
    "\n",
    "# execute the query \n",
    "try: cursor.execute(sql_query)\n",
    "except: print(\"Error in executing query!\")\n",
    "    \n",
    "# capture details of those customers into a dataframe\n",
    "query_result=cursor.fetchall()\n",
    "lst=[]\n",
    "for row in query_result:\n",
    "    print(row)\n",
    "    lst.append(row)\n",
    "df_qres=pd.DataFrame(lst)\n",
    "df_qres.head()\n",
    "\n",
    "# extracting the customers' names from the dataframe to a list\n",
    "names_orig=list(df_qres[\"customer_name\"])\n",
    "\n",
    "len_names_orig=len(names_orig)\n",
    "\n",
    "# replacing spaces with dots\n",
    "names_original_spaces_dotted=[]\n",
    "for i in range(0, len_names_orig):\n",
    "    name=\"\"\n",
    "    names_orig[i]=str(names_orig[i])\n",
    "    for j in range(0, len(names_orig[i])):\n",
    "        if(names_orig[i][j]>='A' and names_orig[i][j]<='Z'): name=name+names_orig[i][j]\n",
    "        else: name=name+\".\"\n",
    "    names_original_spaces_dotted.append(name)\n",
    "\n",
    "# truncating names/appending dots\n",
    "names_original_padded=[] \n",
    "for i in range(0, len_names_orig):\n",
    "    name=\"\"\n",
    "    for j in range(0, 30):\n",
    "        if(j<len(names_original_spaces_dotted[i])): name=name+names_original_spaces_dotted[i][j]\n",
    "        else: name=name+'.'\n",
    "    names_original_padded.append(name)\n",
    "\n",
    "# representing names in terms of ascii values\n",
    "names_ascii=[]\n",
    "for i in range(0, len_names_orig):\n",
    "    name=names_original_padded[i]\n",
    "    name_ascii=[]\n",
    "    for j in range(0, len(name)): name_ascii.append(ord(name[j]))\n",
    "    names_ascii.append(name_ascii)  \n",
    "    \n",
    "# take the preprocessed, real-world names into a form passable to the computation graph and show dimensions\n",
    "X_real_world=np.array(names_ascii).T\n",
    "X_real_world_max = np.max(X_real_world, axis=1, keepdims=True) \n",
    "X_real_world=X_real_world/X_real_world_max\n",
    "\n",
    "print (\"number of real world names = \" + str(X_real_world.shape[1]))      \n",
    "print (\"X_real_world shape: \" + str(X_real_world.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_x, None])\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \n",
    "    # keeping results consistent\n",
    "    tf.set_random_seed(1)                   \n",
    "        \n",
    "    # defining the neural network's architecture\n",
    "    W1 = tf.get_variable(\"W1\", [40, 30], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [40, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [40, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [40, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W4 = tf.get_variable(\"W4\", [10, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable(\"b4\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W5 = tf.get_variable(\"W5\", [5, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b5 = tf.get_variable(\"b5\", [5, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W6 = tf.get_variable(\"W6\", [2, 5], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b6 = tf.get_variable(\"b6\", [2, 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5,\n",
    "                  \"W6\": W6,\n",
    "                  \"b6\": b6}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # retrieving the parameters from the dictionary 'parameters' \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6']\n",
    "                                         \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                                              \n",
    "    A1 = tf.nn.relu(Z1)   \n",
    "    \n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                             \n",
    "    A2 = tf.nn.relu(Z2)  \n",
    "    \n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3) \n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    Z4 = tf.add(tf.matmul(W4, A3), b4)\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    Z5 = tf.add(tf.matmul(W5, A4), b5)\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    \n",
    "    Z6 = tf.add(tf.matmul(W6, A5), b6)\n",
    "    \n",
    "    return Z6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z6, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z6)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.002,\n",
    "          num_epochs = 1000, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # n_x: input size, m : number of examples in the train set\n",
    "    n_y = Y_train.shape[0]                            # n_y: output size\n",
    "    costs = []                                        # to keep track of the cost\n",
    "    \n",
    "    # creating Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # initializing parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # forward propagation: building the forward propagation in the tensorflow graph\n",
    "    Z6 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # cost function: adding cost function to tensorflow graph\n",
    "    cost = compute_cost(Z6, Y)\n",
    "    \n",
    "    # backpropagation: defining the tensorflow AdamOptimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # initializing all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # starting the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # running the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       \n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # selecting a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # printing the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plotting the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # saving the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        print (\"Parameters have been trained!\")\n",
    "        print()\n",
    "\n",
    "        # accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(Z6), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "        \n",
    "        # confusion matrix\n",
    "        predicted0_actual0 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 0), tf.equal(tf.argmax(Y), 0)), \"float\"))\n",
    "        predicted0_actual1 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 0), tf.equal(tf.argmax(Y), 1)), \"float\"))\n",
    "        predicted1_actual0 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 1), tf.equal(tf.argmax(Y), 0)), \"float\"))\n",
    "        predicted1_actual1 = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.argmax(Z6), 1), tf.equal(tf.argmax(Y), 1)), \"float\"))\n",
    "\n",
    "        print(\"True negatives:\", predicted0_actual0.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"False negatives:\", predicted0_actual1.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"False positives:\", predicted1_actual0.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"True positives:\", predicted1_actual1.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "\n",
    "        # precision, recall\n",
    "        recall=tf.divide(predicted1_actual1, tf.add_n([predicted1_actual1, predicted0_actual1]))\n",
    "        precision=tf.divide(predicted1_actual1, tf.add_n([predicted1_actual1, predicted1_actual0]))\n",
    "        \n",
    "        print(\"Train Precision:\", precision.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test Precision:\", precision.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"Train Recall:\", recall.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test Recall:\", recall.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "        \n",
    "        # accuracy on 0s (females)\n",
    "        accuracy_females=tf.divide(predicted0_actual0, tf.add_n([predicted0_actual0, predicted1_actual0]))\n",
    "\n",
    "        print(\"Test accuracy on females:\", accuracy_females.eval({X: X_test, Y: Y_test}))\n",
    "        print()\n",
    "        \n",
    "        # cases where 0s (females) were predicted \n",
    "        predicted0=tf.equal(tf.argmax(Z6), 0)\n",
    "        \n",
    "        predicted_zeros=list(predicted0.eval({X: X_real_world}))\n",
    "        len_predicted_zeros=len(predicted_zeros)\n",
    "        for i in range(0, len_predicted_zeros):\n",
    "            if(predicted_zeros[i]==True): predicted_zeros[i]=\"female\"\n",
    "            else: predicted_zeros[i]=\"male\"\n",
    "                \n",
    "        # add the learned gender to the query result\n",
    "        df_qres['computed_gender']=predicted_zeros\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset out females and output as csv\n",
    "df_qres=df_qres[df_qres.computed_gender==\"female\"]\n",
    "df_qres.to_csv(\"gender_computed_pitch_BTY.csv\", index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
